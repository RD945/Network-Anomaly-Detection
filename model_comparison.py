# -*- coding: utf-8 -*-
"""Model Comparison.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dn4tevdz61jbzcd1lrHCE8BQG2RcmbQo
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pandas.api.types import is_numeric_dtype
import warnings
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.tree  import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import BernoulliNB
from lightgbm import LGBMClassifier
from sklearn.feature_selection import RFE
import itertools
from xgboost import XGBClassifier
from tabulate import tabulate
import time
import optuna
optuna.logging.set_verbosity(optuna.logging.WARNING)

train=pd.read_csv('Train_data_1.csv')

test=pd.read_csv('Test_data_1.csv')

train.head()

train.info()

train.describe()

train.describe(include='object')

train.shape

train.isnull().sum()

total = train.shape[0]
missing_columns = [col for col in train.columns if train[col].isnull().sum() > 0]
for col in missing_columns:
    null_count = train[col].isnull().sum()
    per = (null_count/total) * 100
    print(f"{col}: {null_count} ({round(per, 3)}%)")

print(f"Number of duplicate rows: {train.duplicated().sum()}")

sns.countplot(x=train['class'])

print('Class distribution Training set:')
print(train['class'].value_counts())

def le(df):
    for col in df.columns:
        if df[col].dtype == 'object':
                label_encoder = LabelEncoder()
                df[col] = label_encoder.fit_transform(df[col])

le(train)
le(test)

train.drop(['num_outbound_cmds'], axis=1, inplace=True)
test.drop(['num_outbound_cmds'], axis=1, inplace=True)

train.head()

X_train = train.drop(['class'], axis=1)
Y_train = train['class']

rfc = RandomForestClassifier()

rfe = RFE(rfc, n_features_to_select=10)
rfe = rfe.fit(X_train, Y_train)

feature_map = [(i, v) for i, v in itertools.zip_longest(rfe.get_support(), X_train.columns)]
selected_features = [v for i, v in feature_map if i==True]

selected_features

X_train = X_train[selected_features]

scale = StandardScaler()
X_train = scale.fit_transform(X_train)
test = scale.fit_transform(test)

x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, train_size=0.70, random_state=2)

x_train.shape

x_test.shape

y_train.shape

y_test.shape

from sklearn.linear_model import LogisticRegression

clfl = LogisticRegression(max_iter = 1200000)
start_time = time.time()
clfl.fit(x_train, y_train.values.ravel())
end_time = time.time()
print("Training time: ", end_time-start_time)

start_time = time.time()
y_test_pred = clfl.predict(x_train)
end_time = time.time()
print("Testing time: ", end_time-start_time)

lg_model = LogisticRegression(random_state = 42)
lg_model.fit(x_train, y_train)

lg_train, lg_test = lg_model.score(x_train , y_train), lg_model.score(x_test , y_test)

print(f"Training Score: {lg_train}")
print(f"Test Score: {lg_test}")

def objective(trial):
    n_neighbors = trial.suggest_int('KNN_n_neighbors', 2, 16, log=False)
    classifier_obj = KNeighborsClassifier(n_neighbors=n_neighbors)
    classifier_obj.fit(x_train, y_train)
    accuracy = classifier_obj.score(x_test, y_test)
    return accuracy

study_KNN = optuna.create_study(direction='maximize')
study_KNN.optimize(objective, n_trials=1)
print(study_KNN.best_trial)

KNN_model = KNeighborsClassifier(n_neighbors=study_KNN.best_trial.params['KNN_n_neighbors'])
KNN_model.fit(x_train, y_train)

KNN_train, KNN_test = KNN_model.score(x_train, y_train), KNN_model.score(x_test, y_test)

print(f"Train Score: {KNN_train}")
print(f"Test Score: {KNN_test}")

from sklearn.tree import DecisionTreeClassifier

clfd = DecisionTreeClassifier(criterion ="entropy", max_depth = 4)
start_time = time.time()
clfd.fit(x_train, y_train.values.ravel())
end_time = time.time()
print("Training time: ", end_time-start_time)

start_time = time.time()
y_test_pred = clfd.predict(x_train)
end_time = time.time()
print("Testing time: ", end_time-start_time)

def objective(trial):
    dt_max_depth = trial.suggest_int('dt_max_depth', 2, 32, log=False)
    dt_max_features = trial.suggest_int('dt_max_features', 2, 10, log=False)
    classifier_obj = DecisionTreeClassifier(max_features = dt_max_features, max_depth = dt_max_depth)
    classifier_obj.fit(x_train, y_train)
    accuracy = classifier_obj.score(x_test, y_test)
    return accuracy

study_dt = optuna.create_study(direction='maximize')
study_dt.optimize(objective, n_trials=30)
print(study_dt.best_trial)

dt = DecisionTreeClassifier(max_features = study_dt.best_trial.params['dt_max_features'], max_depth = study_dt.best_trial.params['dt_max_depth'])
dt.fit(x_train, y_train)

dt_train, dt_test = dt.score(x_train, y_train), dt.score(x_test, y_test)

print(f"Train Score: {dt_train}")
print(f"Test Score: {dt_test}")

data = [["KNN", KNN_train, KNN_test],
        ["Logistic Regression", lg_train, lg_test],
        ["Decision Tree", dt_train, dt_test]]

col_names = ["Model", "Train Score", "Test Score"]
print(tabulate(data, headers=col_names, tablefmt="fancy_grid"))

SEED = 42

# Decision Tree Model
dtc = DecisionTreeClassifier()

# KNN
knn = KNeighborsClassifier()

# LOGISTIC REGRESSION MODEL

lr = LogisticRegression()

from sklearn.model_selection import cross_val_score
models = {}
models['KNeighborsClassifier']= knn
models['LogisticRegression']= lr
models['DecisionTreeClassifier']= dtc

scores = {}
for name in models:
  scores[name]={}
  for scorer in ['precision','recall']:
    scores[name][scorer] = cross_val_score(models[name], x_train, y_train, cv=10, scoring=scorer)

def line(name):
  return '*'*(25-len(name)//2)

for name in models:
  print(line(name), name, 'Model Validation', line(name))

  for scorer in ['precision','recall']:
    mean = round(np.mean(scores[name][scorer])*100,2)
    stdev = round(np.std(scores[name][scorer])*100,2)
    print ("Mean {}:".format(scorer),"\n", mean,"%", "+-",stdev)
    print()

for name in models:
    for scorer in ['precision','recall']:
        scores[name][scorer] = scores[name][scorer].mean()
scores=pd.DataFrame(scores).swapaxes("index", "columns")*100
scores.plot(kind = "bar",  ylim=[80,100], figsize=(24,6), rot=0)

models = {}
models['KNeighborsClassifier']= knn
models['LogisticRegression']= lr
models['DecisionTreeClassifier']= dtc

preds={}
for name in models:
    models[name].fit(x_train, y_train)
    preds[name] = models[name].predict(x_test)
print("Predictions complete.")

from sklearn.metrics import confusion_matrix, classification_report, f1_score
def line(name,sym="*"):
    return sym*(25-len(name)//2)
target_names=["normal","anomaly"]
for name in models:
    print(line(name), name, 'Model Testing', line(name))
    print(confusion_matrix(y_test, preds[name]))
    print(line(name,'-'))
    print(classification_report(y_test, preds[name], target_names=target_names))

f1s = {}
for name in models:
    f1s[name]=f1_score(y_test, preds[name])
f1s=pd.DataFrame(f1s.values(),index=f1s.keys(),columns=["F1-score"])*100
f1s.plot(kind = "bar",  ylim=[80,100], figsize=(10,6), rot=0)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.naive_bayes import GaussianNB

import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
from tabulate import tabulate

# Initialize dictionary to store metrics for each model
model_metrics = {
    'Model': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1 Score': [],
    'Average Score': []  # Column for the average score
}

# Define models dictionary, ensure all models are listed
models = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(),
    'SVM': SVC(),
    'KNN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(),
    'Naive Bayes': GaussianNB(),
    # Add any additional models here
}

# Fit and evaluate each model
for name, model in models.items():
    model.fit(x_train, y_train)  # Train the model
    y_pred = model.predict(x_test)  # Make predictions

    # Compute each metric
    accuracy = accuracy_score(y_test, y_pred) * 100
    precision = precision_score(y_test, y_pred, pos_label=1) * 100
    recall = recall_score(y_test, y_pred, pos_label=1) * 100
    f1 = f1_score(y_test, y_pred, pos_label=1) * 100

    # Append model name and metrics to the dictionary
    model_metrics['Model'].append(name)
    model_metrics['Accuracy'].append(accuracy)
    model_metrics['Precision'].append(precision)
    model_metrics['Recall'].append(recall)
    model_metrics['F1 Score'].append(f1)

    # Calculate the average score and add to dictionary
    avg_score = (accuracy + precision + recall + f1) / 4
    model_metrics['Average Score'].append(avg_score)

# Create a DataFrame to display the metrics for each model
metrics_df = pd.DataFrame(model_metrics)

# Display metrics table
print("Model Comparison Table:")
print(tabulate(metrics_df, headers='keys', tablefmt='fancy_grid'))

# Set style and plot the bar graph
plt.style.use('ggplot')

# Increase the figure size for an even larger graph
plt.figure(figsize=(24, 12))  # Adjust the size further for a larger plot

# Plot metrics with custom colors and labels
ax = metrics_df.set_index('Model').iloc[:, :-1].plot(kind='bar', color=['skyblue', 'salmon', 'lightgreen', 'gold'],
                                                     figsize=(24, 12), ylim=(0, 100), edgecolor='black', width=0.9)  # Increased width

# Title with larger font size and adjusted position
plt.title("Model Performance Comparison", fontsize=26, weight='bold', pad=30)  # Increased font size and padding for title
plt.ylabel("Score (%)", fontsize=20)  # Increased font size for ylabel
plt.xlabel("Model", fontsize=20)  # Increased font size for xlabel
plt.xticks(rotation=45, fontsize=16)  # Increased font size for x-axis labels
plt.yticks(fontsize=16)  # Increased font size for y-axis labels
plt.legend(loc="upper right", fontsize=16, title="Metrics")

# Adding data labels on each bar for readability
for container in ax.containers:
    ax.bar_label(container, fmt='%.2f', label_type='edge', fontsize=14)  # Increased font size for data labels

# Show the plot
plt.tight_layout()  # Adjust layout to prevent clipping of labels and title
plt.show()

# Identifying the best model for each metric
best_accuracy_model = metrics_df.loc[metrics_df['Accuracy'].idxmax(), 'Model']
best_accuracy_score = metrics_df['Accuracy'].max()

best_precision_model = metrics_df.loc[metrics_df['Precision'].idxmax(), 'Model']
best_precision_score = metrics_df['Precision'].max()

best_recall_model = metrics_df.loc[metrics_df['Recall'].idxmax(), 'Model']
best_recall_score = metrics_df['Recall'].max()

best_f1_model = metrics_df.loc[metrics_df['F1 Score'].idxmax(), 'Model']
best_f1_score = metrics_df['F1 Score'].max()

# Identify the best model based on the average score
best_average_index = metrics_df['Average Score'].idxmax()
best_average_model = metrics_df['Model'][best_average_index]
best_average_score = metrics_df['Average Score'].max()

# Printing the best model for each metric and the average score
print(f"\nBest Model by Accuracy: {best_accuracy_model} with an Accuracy of {best_accuracy_score:.2f}%")
print(f"Best Model by Precision: {best_precision_model} with a Precision of {best_precision_score:.2f}%")
print(f"Best Model by Recall: {best_recall_model} with a Recall of {best_recall_score:.2f}%")
print(f"Best Model by F1 Score: {best_f1_model} with an F1 Score of {best_f1_score:.2f}%")
print(f"Best Model Overall: {best_average_model} with an Average Score of {best_average_score:.2f}%")